
params{
        root_folder=""
        conda_envs_path="" // installation directory of local enviroment environmen if singulariy is not used
        container_path="" // by default diretetyl pull from the library website, or can be provided here 
    
        query_currentSpe_TaxID = "511145"
        query_current_EggNOG_maxLevel = "1224"
        subject1_currentSpe_TaxID = "1274374"
        subject1_current_EggNOG_maxLevel = "1239"
        subject2_currentSpe_TaxID = "105422"
        subject2_current_EggNOG_maxLevel = "201174"
        subject3_currentSpe_TaxID = "411476"
        subject3_current_EggNOG_maxLevel = "976"
        Nf90_thres = "16"
    
        // here specify the number of cpus for nextflow process, small, middle, large correspond to processes need more cpus
        // here set as minium to fit to most system,but for the fast computation, more cpus could be specified
        small_cpu_nums = 1 // 8, 1
        middle_cpu_nums = 1 // 30,1
        large_cpu_nums = 1  // 100, 1
    
    
        // here specify the memeroy  for nextflow process, small, middle, large correspond to processes need more memory
        // here set as minium to fit to most system,but for the fast computation, larger memory need to be specired
        small_memory = 4.GB   //128, 32.GB 
        middle_memory = 4.GB  // 128,,32.GB 
        large_memory = 4.GB  // 300 ,32.GB  make it as dynamic setting later ?
    
        // here correspond to python multiprocessing process
        small_mp_task_nums = "10"  //"10"
        middle_mp_task_nums = "10"  //"30"
        large_mp_task_nums = "10"   //"50"
    
        //since there are millions of PPI to be computerd , here split them into "DCA_blockNum" chunks 
        //Each chunk will be computed within one process 
        DCA_blockNum = 3 //10, when testing, keep it small 
        singleDCA_memory= 32.GB // Memory size needed for DCA computation, for long protein, this number may need to be increased
    
        code_utilities_folder = "${projectDir}/../src/utilities/"
        phmmer_path = "phmmer"
        clustalo_path = "clustalo"
        hmmbuild_path = "hmmbuild" 
        hmmalign_path = "hmmalign"
        blastp_path = "blastp"
    

        
}


profiles {
    
    standard { 
        // this one passed test ,local executor
        workDir="${params.root_folder}/nextflow/work" 
        conda.enabled = true  //this line is necesseary,  https://www.nextflow.io/docs/latest/conda.html

        includeConfig 'conf/standard.config'    
    }
    
    

    singularity {
        // this one passed test , singularity with local executor
        
        singularity.enabled    = true
        singularity.autoMounts = true        
        singularity.runOptions = "--bind ${params.root_folder}" // this give permission to singularity to mount ths folder https://github.com/nextflow-io/nextflow/discussions/2567,https://www.nextflow.io/docs/latest/singularity.html
        
        conda.enabled = false
        
        workDir="${params.root_folder}/nextflow/work"   //this line is necesseary,  https://www.nextflow.io/docs/latest/conda.html
        
        includeConfig 'conf/standard.config'    
    }
    
    
    slurm    { 
        
        conda.enabled = true //this line is necesseary,  https://www.nextflow.io/docs/latest/conda.html
        singularity.enabled    = false
        
        workDir="${params.root_folder}/nextflow/work" // for s3it. its "/shares/von-mering.imls.uzh/tao"
        
        

        //can we put this here ?, if so later can also just load standard profile 
        process {
            executor = 'slurm'
            time=48.h
        }
        
        executor {
            // queueSize = 100
            queueSize = 10
            submitRateLimit = '5 sec'
        }
        
        includeConfig 'conf/standard.config' 
        // includeConfig 'conf/slurm.config'    
        //https://github.com/nf-core/radseq/blob/bff60aca8031af5d59284ab337962437bc1b543c/nextflow.config
    } 

    
    
    slurm_withSingularity    { 
         
        singularity.enabled    = true
        singularity.autoMounts = true        
        singularity.runOptions = "--bind ${params.root_folder}" // by default /shares at s3it is mouted already, but no hurm 
        
        conda.enabled = false //this line is necesseary,  https://www.nextflow.io/docs/latest/conda.html
        
        workDir="${params.root_folder}/nextflow/work" // for s3it. its "/shares/von-mering.imls.uzh/tao"
        
        

        //can we put this here ?, if so later can also just load standard profile 
        process {
            executor = 'slurm'
            time=48.h
        }
        
        executor {
            // queueSize = 100
            queueSize = 10
            submitRateLimit = '5 sec'
        }
        
        includeConfig 'conf/standard.config'    
    } 
    
    
    
    
}






