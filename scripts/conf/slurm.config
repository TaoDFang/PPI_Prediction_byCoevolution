// env.NXF_OPTS="-Xms500M -Xmx2G" 

// if run from s3it singulartiy teminal, the path is without "/net/cephfs/", actullay now also okay without /net/cephfs/ from normal terminal
workDir="/shares/von-mering.imls.uzh/tao/nextflow/work" 
conda.enabled = true  //this line is necesseary,  https://www.nextflow.io/docs/latest/conda.html

params {
    // Config options
    RawData_Folder  = "/shares/von-mering.imls.uzh/tao/nextflow/STRING_Data_11.5"
    PPI_Coevolution = "/shares/von-mering.imls.uzh/tao/nextflow/PPI_Coevolution"
    
    nf_training_conda="/data/tfang/conda-envs/nf-training"
    pydca_conda="/data/tfang/conda-envs/py37_pydca"
    sequence_tools_conda = "/data/tfang/conda-envs/sequence_tools"

}


// setup SLURM 
process {
    // conda.enabled = true  //this line is necesseary,  https://www.nextflow.io/docs/latest/conda.html
    // executor = 'local'  
    // process.executor = 'local'   
    executor = 'slurm'
    time=24.h
//     withLabel:process_high_memory {clusterOptions = '--qos=himem --partition=himem'}
//     withLabel:'!process_high_memory' {clusterOptions = '--qos=general --partition=general'}
    

    withLabel: simple_process {
        cpus = 4
        memory = 64.GB
        // queue="standard" // this is cpu partition in s3it https://docs.s3it.uzh.ch/cluster/resources/
        // clusterOptions = '--partition=standard --error=${workDir}/../slurm_reports/error_%A_%a.txt'
        // {clusterOptions = '--partition=standard'} // this linke cause GB problem , why
        conda="${params.sequence_tools_conda}"
        // conda="${params.nf_training_conda}"
        // set log folder here  or nextflow level ??
        // conda="/mnt/mnemo5/tao/anaconda3/envs/nf-training"
    }
    
    withLabel: simple_py_process {
        cpus = 8
        memory = 128.GB
        conda="${params.sequence_tools_conda}"
        
    }
    
    
    withLabel: many_cpu_process {
        cpus = 30 //60
        memory = 100.GB
        conda="${params.sequence_tools_conda}"
    }
    
    withLabel: large_memory_process {
        cpus = 30
        memory = 300.GB //srun --pty -n 1 -c 30 --time=120:00:00 --mem=100G bash -l bash -l, Process requirement exceeds available memory -- req: 400 GB; avail: 376.5 GB
        // memory = 400.GB
        conda="${params.sequence_tools_conda}"
    }
    
    withLabel: manyCPU_largeMemory_process {
        cpus = 30 //100, 60
        memory = 300.GB
        conda="${params.sequence_tools_conda}"
    }
    
    withLabel: coevolutionComputation_mfDCA_process {
        cpus = 30 //100, ??!!! here need to think how many cpu to give in slurm executor mode  
        memory = 300.GB
        conda="${params.pydca_conda}"
        // conda="/mnt/mnemo5/tao/anaconda3/envs/py37_pydca"
    }  
    
    
    
//     withName: prepareFastaDataBySpecies {
//         cpus=32
//         memory=100.GB
//         conda="${params.sequence_tools_conda}"
//         //conda="/mnt/mnemo5/tao/anaconda3/envs/nf-training"
//     }
    
    
//     withName: moveOnlyBacteriaSepcies {
//         cpus=32
//         memory=100.GB
//         conda="${params.sequence_tools_conda}"
//         //conda="/mnt/mnemo5/tao/anaconda3/envs/nf-training"    
//     }
      


}



executor {
    // queueSize = 100
    queueSize = 10
    submitRateLimit = '5 sec'
}

